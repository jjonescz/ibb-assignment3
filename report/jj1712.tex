% ============================================================================================
% This is a LaTeX template used for the course
%
%  I M A G E   B A S E D   B I O M E T R I C S
%
% Faculty of Computer and Information Science
% University of Ljubljana
% Slovenia, EU
%
% You can use this template for whatever reason you like.
% If you have any questions feel free to contact
% ziga.emersic@fri.uni-lj.si
% ============================================================================================

\documentclass[9pt]{IEEEtran}

% basic
\usepackage[english]{babel}
\usepackage{graphicx,epstopdf,fancyhdr,amsmath,amsthm,amssymb,url,array,textcomp,svg,listings,hyperref,xcolor,colortbl,float,gensymb,longtable,supertabular,multicol,placeins}

 % `sumniki' in names
\usepackage[utf8x]{inputenc}

 % search and copy for `sumniki'
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\input{glyphtounicode}
\pdfgentounicode=1

% tidy figures
\graphicspath{{./figures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg,.eps}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor trig-gs}

% ============================================================================================

\title{\vspace{0ex} %
Ear Recognition
\\ \large{Assignment \#3}\\ \normalsize{Image Based Biometrics 2020/21, Faculty of Computer and Information Science, University of Ljubljana}}
\author{ %
Jan~Jone≈°
\vspace{-4.0ex}
}

% ============================================================================================

\begin{document}

\maketitle

\begin{abstract}
    Ears as biometric modality for identity recognition has several advantages.
    This modality is universal and easily obtainable from images of people.
    On the other hand, focus of image-based biometric research lies elsewhere (e.g., face recognition).
    Therefore, high-quality datasets consisting of segmented and labeled ear images are scarce.
    Furthermore, deep learning techniques evolve quickly and they provide state-of-the-art solutions for wide range of image classification tasks.
    These techniques are certainly applicable to image-based biometry and ear recognition is one of those less researched.
    In this paper we train deep neural network classifier on readily-available annotated ear dataset.
    Due to low amount of data points available for training, we build our network upon model that is pre-trained on large image dataset from another domain.
    We are able to achieve accuracy 19.60~\% on unseen test data.
\end{abstract}

\section{Introduction}

Convolutional neural networks (CNNs) are deep neural networks that are state-of-the-art approach to solving various image-based tasks including segmentation and classification.
They have also been recently employed for ear recognition~\cite{emersic2017}.

In this paper we build and evaluate CNN model intended to be used as recognition stage in biometric pipeline with ear modality.
Our model accepts image of an ear at its input and produces identification of the subject at its output.
Input image is expected to be cropped, so that only the ear is visible.
For that, e.g., a separate CNN-based ear detector can be used as a previous stage in the biometric pipeline and then the whole pipeline can accept images of people as inputs.

Since ears are not prevalent modality in image-based authentication, high-quality datasets containing annotated ear images are scarce and they usually consist of relatively few datapoints.
We approach this problem in two ways:
\begin{enumerate}
    \item We build our model upon publicly available CNN model pre-trained for image classification.
          This model has been trained on datasets much larger than ours.
          We use its weights as our starting point and employ transfer learning to adapt this model for ear image classification.
    \item We analyze impact of image augmentation on the ear dataset used for training our CNN model.
          We hypothesize that image augmentation could improve the model's ability to generalize even from relatively small dataset because the input data are more varied.
\end{enumerate}

\section{Methodology}
The chosen dataset is already split into training and testing subsets (750 and 250 images, respectively).
I have further subdivided the original training data into (actual) training and development subsets (500 and 250 images, respectively).
Both of these training subsets were used during development of the detector.
Testing data were used only during final evaluation.

All images are already resized to the same size ($480 \times 360$ pixels) in the dataset.
In order to use them easily in convolutional layers of the CNN, I have resized them to dimensions divisible by 32 ($480 \times 352$ pixels) during preprocessing.

The CNN uses pre-trained EfficientNet-B0 network~\cite{efficientNet} as encoder in U-Net-like architecture~\cite{unet} with custom decoder and segmentation head.
EfficientNet-B0 takes pixels as input and outputs them scaled down 32 times. The decoder uses transposed convolutions to upsample these features back to the original resolution and employs skip layers to pass information from the corresponding layer of the encoder. Segmentation head consists of transposed convolution and sigmoid activation layer.

The encoder has its weights fixed to the pre-trained values. Only decoder's weights are trained. The CNN has 6.9 million trained parameters and 4.0 million fixed parameters.

\section{Results}
I have used intersection over union (IoU) to evaluate model's performance.
To train the CNN, I have used BCE-Dice loss function.
This loss was optimized using Adam optimizer with standard learning rate 0.001.

From evolution of these metrics during training (Figure~\ref{fig:evo}), I~would ideally conclude number of epochs for training where loss would stop decreasing.
Unfortunately, neither loss nor accuracy converged after 35 epochs and I did not posses computing power where evaluating more epochs would be feasible.
Therefore I chose 35 as number of epochs for training noting that larger number would probably yield even better results.

\begin{figure}[ht]
    \centering
    % \includegraphics[width=0.49\columnwidth]{loss}
    % \includegraphics[width=0.49\columnwidth]{iou}
    \caption{Evolution of metrics during training on training and development datasets.}
    \label{fig:evo}
\end{figure}

Three best (97.9~\%, 97.8~\%, 97.7~\%) and three worst (22.7~\%, 33.1~\%, 61.5~\%) IoU results in development data can be seen in Figure~\ref{fig:examples}.
The worst IoU results illustrate some problems the network might have, i.e., grayscale images, small faces (due to more people in the picture) and earrings, respectively.

\begin{figure}[H]
    \centering
    % \includegraphics[width=1\columnwidth]{examples-best}\\
    \vspace{0.5cm}
    % \includegraphics[width=1\columnwidth]{examples-worst}
    \caption{Three best (top) and three worst (bottom) segmentation results in development dataset.}
    \label{fig:examples}
\end{figure}

Overall distribution of IoU in development dataset is depicted in Figure~\ref{fig:hist}.
It can be seen that most data are segmented very well (IoU more than 90~\%).

\begin{figure}
    \centering
    % \includegraphics[width=1\columnwidth]{iou-hist}
    \caption{Histogram of IoU in development dataset.}
    \label{fig:hist}
\end{figure}

Final mean IoU on training data was 92.7~\%, mean IoU on development data was 91.5~\% and mean IoU on testing data was 83.2~\%.

\section{Conclusion}
The trained CNN shows very good results as is and there is also some space for future improvement.
Image augmentation would help making the training dataset more diverse (e.g., horizontal/vertical flips, color transformations, cutouts, etc.).
Fine-tuning (a process where the pre-trained EfficientNet is also trained with low learning rate) usually also improves performance.
Finally, architecture of the CNN could be more complex (e.g., higher-level EfficientNet could be used) but that would also make training more computing-intensive and hence slower.

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

\end{document}
